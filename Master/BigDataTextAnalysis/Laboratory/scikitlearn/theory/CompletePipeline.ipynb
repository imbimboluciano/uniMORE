{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"provenance":[],"collapsed_sections":["NdUsFzCOopVd","6nDMyu1_opWu"],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"YQ9xpQO3opTw"},"source":["# Pipeline Complete"]},{"cell_type":"code","metadata":{"id":"38LPRHlBopTx"},"source":["import sys\n","import os\n","import pandas as pd\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","\n","import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lxbc3j19opT1"},"source":["# Always good to set a seed for reproducibility\n","SEED = 7\n","np.random.seed(SEED)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"erAI56vKopT4"},"source":["# Loading Data\n","df = pd.read_csv('diabetes.csv')\n","\n","# Getting dataframe columns names\n","df_name = df.columns\n","print(df_name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head()"],"metadata":{"id":"hWrcnqn4INOi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.shape"],"metadata":{"id":"YOICJ16SIkMS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"CbOi4PeAopT8"},"source":["## Analyze Data"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"2Ld1rb4xopT9"},"source":["### Descriptive Statitics"]},{"cell_type":"code","metadata":{"id":"3KJ15FYvopT-"},"source":["df.info()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"_2XOJsSCopUB"},"source":["The data is not included time or object.\n","\n","There is no null value in data set."]},{"cell_type":"code","metadata":{"id":"imZj6wvqopUF"},"source":["df.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"08SsXuyCkMbI"},"source":["df['Outcome'].unique()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WANaDOncopUJ"},"source":["### Data Visualization"]},{"cell_type":"code","metadata":{"id":"9Opim8fhopUK"},"source":["g = sns.pairplot(df, hue=\"Outcome\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"vFzinCKlopUO"},"source":["It seems that the data suffer from outliers\n","\n","Let's see for example pregnancy distribution"]},{"cell_type":"code","metadata":{"id":"BEu_jHYjopUP"},"source":["df[\"Pregnancies\"].hist()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"hF68lR98opUS"},"source":["17 times pregnancy is a little bite strange, is it outlier ?"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"tyvF6vQIopUT"},"source":["Let's investigate each feature distribution for each outcome\n","\n","A fundamental task in many statistical analyses is to characterize the location and variability of a data set. A further characterization of the data includes skewness and kurtosis.\n","\n","__Skewness__ is a measure of symmetry, or more precisely, the lack of symmetry. A distribution, or data set, is symmetric if it looks the same to the left and right of the center point.\n","- Negative values indicate data that are __skewed left__, i.e., the left tail is long relative to the right tail.\n","- Positive values indicate data that are __skewed right__, i.e., the right tail is long relative to the left tail.\n","\n","__Kurtosis__ is a measure of whether the data are heavy-tailed or light-tailed relative to a normal distribution. That is, data sets with high kurtosis tend to have heavy tails, or outliers. Data sets with low kurtosis tend to have light tails, or lack of outliers. A uniform distribution would be the extreme case"]},{"cell_type":"code","metadata":{"id":"kkYl0nSOopUT"},"source":["target = \"Outcome\"\n","\n","for i in range(len(df_name[:-1])):\n","    plt.figure()\n","    df.groupby(target)[df_name[i]].plot(kind='hist',alpha=0.8,legend=True,title=df_name[i])"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from scipy.stats import skew\n","from scipy.stats import kurtosis\n","\n","for i in range(len(df_name)):\n","    print('-*-'*25)\n","    print(df_name[i])\n","    print(\"{0} mean : \".format(df_name[i]), np.mean(df[df_name[i]]))\n","    print(\"{0} var  : \".format(df_name[i]), np.var(df[df_name[i]]))\n","    print(\"{0} skew : \".format(df_name[i]), skew(df[df_name[i]]))\n","    print(\"{0} kurt : \".format(df_name[i]), kurtosis(df[df_name[i]])) # >0 (curva più appuntita), <0 (curva più piatta)"],"metadata":{"id":"d25ssQtjlN2l"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XSLzySbOopUX"},"source":["print(df.Outcome.value_counts())\n","sns.countplot(df, x='Outcome')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"dCh6o4-ZopUf"},"source":["In the case of classification problem we always need to check the target distribution. If the distribution of target is not balance, we must treat our data more carefully. For example we can use several methods to resampling our data.In addition, we need to use stratified method in our validation in order to keep the same distribution in our train and test."]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"kfNBwX4DopUf"},"source":["### Outliers investigation"]},{"cell_type":"code","metadata":{"id":"KQD8GRWcopUg"},"source":["df.plot(kind='box', subplots=True, layout=(3,3))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o5A6HI1FopUj"},"source":["# for i in range(len(df_name[:-1])):\n","#     plt.figure()\n","#     df[df_name[i]].plot(kind='box')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"tuD8ByHZopUm"},"source":["### Evaluate Algorithms: Baseline\n","We usually devide the data to train and test set. We will not touch test set until the end of the computation and the final perpormance evaluation. Then, we can devide the train set to train and validation sets. We use the validation data set to tune the model.\n","\n","Traditional train test method suffer from high variance test problem. It means that by changing the test set the result of the prediction changes. To overcome this problem we use k-fold validation method in our train and validation set"]},{"cell_type":"code","metadata":{"id":"5Cs1pJ_YopUm"},"source":["from pandas import set_option\n","#from pandas.tools.plotting import scatter_matrix\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import accuracy_score\n","from sklearn.pipeline import Pipeline\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.svm import SVC\n","from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import ExtraTreesClassifier"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"XWB_tNRNopUp"},"source":["Let's make train-validation and test data sets.\n","\n","Note that stratify is used because we want to split the dataset by preserving the percentage of samples for each class."]},{"cell_type":"code","metadata":{"id":"srw0dSb2bUNh"},"source":["df_name[8]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gHrYJJQEopUq"},"source":["X =  df[df_name[0:8]]\n","Y = df[df_name[8]]\n","X_train, X_test, y_train, y_test =train_test_split(X,Y,\n","                                                   test_size=0.25,\n","                                                   random_state=0,\n","                                                   stratify=df['Outcome'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8bYaCqp1opUt"},"source":["# Spot-Check Algorithms\n","def GetBasedModel():\n","    basedModels = []\n","    basedModels.append(('LR'   , LogisticRegression()))\n","    basedModels.append(('LDA'  , LinearDiscriminantAnalysis()))\n","    basedModels.append(('KNN'  , KNeighborsClassifier()))\n","    basedModels.append(('CART' , DecisionTreeClassifier()))\n","    basedModels.append(('NB'   , GaussianNB()))\n","    basedModels.append(('SVM'  , SVC(probability=True)))\n","    basedModels.append(('AB'   , AdaBoostClassifier()))\n","    basedModels.append(('GBM'  , GradientBoostingClassifier()))\n","    basedModels.append(('RF'   , RandomForestClassifier()))\n","    basedModels.append(('ET'   , ExtraTreesClassifier()))\n","\n","    return basedModels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_UIEI4K1opUx"},"source":["def BasedLine2(X_train, y_train, models):\n","    # Test options and evaluation metric\n","    num_folds = 10\n","    scoring = 'accuracy'\n","\n","    results = []\n","    names = []\n","    for name, model in models:\n","        kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=SEED)\n","        cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n","\n","        results.append(cv_results)\n","        names.append(name)\n","        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n","        print(msg)\n","\n","    return names, results"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qq3_jjUoopUz"},"source":["models = GetBasedModel()\n","names, results = BasedLine2(X_train, y_train, models)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cy_CeoLVm7-M"},"source":["models"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EUjJOepSNqu1"},"source":["names"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results"],"metadata":{"id":"AFceS0WBwgyc"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m_GHQMtMopU2"},"source":["comparison = pd.DataFrame(np.array(results).T, columns=names)\n","comparison"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-lz-CxyzopU5"},"source":["comparison.plot(kind='box')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eKECkB1QopU7"},"source":["def ScoreDataFrame(names, results):\n","    def floatingDecimals(f_val, dec=3):\n","        prc = \"{:.\"+str(dec)+\"f}\"\n","        return float(prc.format(f_val))\n","\n","    scores = []\n","    for r in results:\n","        scores.append(floatingDecimals(r.mean(), 4))\n","\n","    scoreDataFrame = pd.DataFrame({'Model':names, 'Score': scores})\n","    return scoreDataFrame"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"631OwWFhopU-"},"source":["basedLineScore = ScoreDataFrame(names,results)\n","basedLineScore"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"E7nqqsecopVB"},"source":["### Feature Engineering\n","#### Data Preprocessig\n","Numerical features preprocessing is different for tree and non tree model.\n","\n","1) Usually:\n","\n","    - Tree based models does not depend on scaling\n","    - Non-tree based models hugely depend on scaling\n","\n","2) Most Often used preprocessing are:\n","\n","    - MinMax scaler to [0,1]\n","    - Standard Scaler to mean = 0 and std =1\n","    - Rank (We do not work on it in this data set)\n","    - Using np.log(1+data), np.sqrt(data) and stats.boxcox(data) (for exp dependency)\n","\n","let's try some of them and see how our model prediction change by scaling"]},{"cell_type":"code","metadata":{"id":"Pgu5ppG0opVB"},"source":["from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import MinMaxScaler\n","\n","\n","def GetScaledModel(nameOfScaler):\n","\n","    if nameOfScaler == 'standard':\n","        scaler = StandardScaler()\n","    elif nameOfScaler =='minmax':\n","        scaler = MinMaxScaler()\n","\n","    pipelines = []\n","    pipelines.append((nameOfScaler+'LR'  , Pipeline([('Scaler', scaler),('LR'  , LogisticRegression())])))\n","    pipelines.append((nameOfScaler+'LDA' , Pipeline([('Scaler', scaler),('LDA' , LinearDiscriminantAnalysis())])))\n","    pipelines.append((nameOfScaler+'KNN' , Pipeline([('Scaler', scaler),('KNN' , KNeighborsClassifier())])))\n","    pipelines.append((nameOfScaler+'CART', Pipeline([('Scaler', scaler),('CART', DecisionTreeClassifier())])))\n","    pipelines.append((nameOfScaler+'NB'  , Pipeline([('Scaler', scaler),('NB'  , GaussianNB())])))\n","    pipelines.append((nameOfScaler+'SVM' , Pipeline([('Scaler', scaler),('SVM' , SVC())])))\n","    pipelines.append((nameOfScaler+'AB'  , Pipeline([('Scaler', scaler),('AB'  , AdaBoostClassifier())])  ))\n","    pipelines.append((nameOfScaler+'GBM' , Pipeline([('Scaler', scaler),('GMB' , GradientBoostingClassifier())])  ))\n","    pipelines.append((nameOfScaler+'RF'  , Pipeline([('Scaler', scaler),('RF'  , RandomForestClassifier())])  ))\n","    pipelines.append((nameOfScaler+'ET'  , Pipeline([('Scaler', scaler),('ET'  , ExtraTreesClassifier())])  ))\n","\n","\n","    return pipelines"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"PWcma8SmopVE"},"source":["### Standard"]},{"cell_type":"code","metadata":{"id":"MkuvK0F7opVF"},"source":["models = GetScaledModel('standard')\n","names,results = BasedLine2(X_train, y_train, models)\n","comparison = pd.DataFrame(np.array(results).T, columns=names)\n","\n","print(comparison.head())\n","comparison.plot(kind='box')\n","\n","scaledScoreStandard = ScoreDataFrame(names,results)\n","compareModels = pd.concat([basedLineScore,\n","                           scaledScoreStandard], axis=1)\n","compareModels"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"zwHBml5zopVI"},"source":["### MinMax"]},{"cell_type":"code","metadata":{"id":"vCvapJh4opVJ"},"source":["models = GetScaledModel('minmax')\n","names,results = BasedLine2(X_train, y_train,models)\n","comparison = pd.DataFrame(np.array(results).T, columns=names)\n","\n","print(comparison.head())\n","comparison.plot(kind='box')\n","\n","scaledScoreMinMax = ScoreDataFrame(names,results)\n","compareModels = pd.concat([basedLineScore,\n","                           scaledScoreStandard,\n","                          scaledScoreMinMax], axis=1)\n","compareModels"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"4t3_HlO9opVL"},"source":["As we can see, the standarscaler and min and max effect on non tree models and the prediction results improve"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"yikXzTyoopVM"},"source":["### Remove Outliers\n","let's remove outliers and see how it effects on the prediction"]},{"cell_type":"code","metadata":{"id":"jS_OQdo3opVM"},"source":["df_t = df.copy()\n","df_t_name = df_t.columns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KMRKXOcPopVP"},"source":["def TurkyOutliers(df_out,nameOfFeature,drop=False):\n","\n","    valueOfFeature = df_out[nameOfFeature]\n","    # Calculate Q1 (25th percentile of the data) for the given feature\n","    Q1 = np.percentile(valueOfFeature, 25.)\n","\n","    # Calculate Q3 (75th percentile of the data) for the given feature\n","    Q3 = np.percentile(valueOfFeature, 75.)\n","\n","    # Use the interquartile range to calculate an outlier step (1.5 times the interquartile range)\n","    step = (Q3-Q1)*1.5\n","    # print \"Outlier step:\", step\n","    outliers = valueOfFeature[~((valueOfFeature >= Q1 - step) & (valueOfFeature <= Q3 + step))].index.tolist()\n","    feature_outliers = valueOfFeature[~((valueOfFeature >= Q1 - step) & (valueOfFeature <= Q3 + step))].values\n","    # df[~((df[nameOfFeature] >= Q1 - step) & (df[nameOfFeature] <= Q3 + step))]\n","\n","\n","    # Remove the outliers, if any were specified\n","    print (\"Number of outliers (inc duplicates): {} and outliers: {}\".format(len(outliers), feature_outliers))\n","    if drop:\n","        good_data = df_out.drop(df_out.index[outliers]).reset_index(drop = True)\n","        print (\"New dataset with removed outliers has {} samples with {} features each.\".format(*good_data.shape))\n","        return good_data\n","    else:\n","        print (\"Nothing happens, df.shape = \",df_out.shape)\n","        return df_out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"seXoib9wpt9i"},"source":["df_t_name"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A7ksoOAOopVR"},"source":["#feature_number = 0\n","#OutLiersBox(df,df_name[feature_number])\n","\n","for i in range(len(df_name[:-1])):\n","    plt.figure()\n","    df[df_name[i]].plot(kind='box')\n","\n","    df_clean = TurkyOutliers(df, df_name[i], True)\n","\n","    plt.figure()\n","    df_clean[df_name[i]].plot(kind='box')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hZU9qqqyECcx"},"source":["df_clean = df.copy()\n","for i in range(len(df_name[:-1])):\n","    print(df_name[i])\n","    df_clean = TurkyOutliers(df_clean,df_name[i],True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tF9fy_ghopVU"},"source":["print('df shape: {}, new df shape: {}, we lost {} rows, {}% of our data'.format(df.shape[0],df_clean.shape[0],\n","                                                              df.shape[0]-df_clean.shape[0],\n","                                                        (df.shape[0]-df_clean.shape[0])/df.shape[0]*100))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"ucHmMAIhopVX"},"source":["### Comparing the accuracy of models after cleaning"]},{"cell_type":"code","metadata":{"id":"y7Xv1rQDopVX"},"source":["df_clean_name = df_clean.columns\n","X_c =  df_clean[df_clean_name[0:8]]\n","Y_c = df_clean[df_clean_name[8]]\n","\n","X_train_c, X_test_c, y_train_c, y_test_c =train_test_split(X_c,Y_c,\n","                                                   test_size=0.25,\n","                                                   random_state=0,\n","                                                   stratify=df_clean['Outcome'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AAT_m-uCopVa"},"source":["models = GetScaledModel('minmax')\n","names,results = BasedLine2(X_train_c, y_train_c,models)\n","comparison = pd.DataFrame(np.array(results).T, columns=names)\n","print(comparison.head())\n","comparison.plot(kind='box')\n","\n","scaledScoreMinMax_c = ScoreDataFrame(names,results)\n","compareModels = pd.concat([basedLineScore,\n","                           scaledScoreStandard,\n","                          scaledScoreMinMax,\n","                          scaledScoreMinMax_c], axis=1)\n","compareModels"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"oh69UlljopVc"},"source":["It can be seen that the prediction is improving."]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"NdUsFzCOopVd"},"source":["### Feature Selection\n","Feature selection is also called variable selection or attribute selection. It is the automatic selection of attributes in your data (such as columns in tabular data) that are most relevant to the predictive modeling problem you are working on.\n","\n","Feature selection methods aid you in your mission to create an accurate predictive model. They help you by choosing features that will give you as good or better accuracy whilst requiring less data.\n","\n","Feature selection methods can be used to identify and remove unneeded, irrelevant and redundant attributes from data that do not contribute to the accuracy of a predictive model or may in fact decrease the accuracy of the model."]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"09kdrhKmopVd"},"source":["### Correlation\n","If we fit highly corrolated data in our model, it results in the overfitting probelm. Thus, for example if there are two highly corrolated features we have to drop the one that has more corrolation with other feature."]},{"cell_type":"code","metadata":{"id":"WEgINJcIopVe"},"source":["sns.heatmap(df.corr(), annot=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"muFrJ5EaopVh"},"source":["There is not highly corrolated feature in this data set."]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"Ngfuztv-opVh"},"source":["### Feature Importance\n","Bagged decision trees like Random Forest and Extra Trees can be used to estimate the importance of features."]},{"cell_type":"code","metadata":{"id":"U6NByb1sopVi"},"source":["clf = ExtraTreesClassifier(n_estimators=250,\n","                              random_state=SEED)\n","\n","clf.fit(X_train_c, y_train_c)\n","\n","# #############################################################################\n","# Plot feature importance\n","feature_importance = clf.feature_importances_\n","# make importances relative to max importance\n","feature_importance = 100.0 * (feature_importance / feature_importance.max())\n","sorted_idx = np.argsort(feature_importance)\n","\n","pos = np.arange(sorted_idx.shape[0]) + .5\n","# plt.subplot(1, 2, 2)\n","plt.barh(pos, feature_importance[sorted_idx], align='center')\n","plt.yticks(pos, df.columns[sorted_idx])#boston.feature_names[sorted_idx])\n","plt.xlabel('Relative Importance')\n","plt.title('Variable Importance')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U1_VpSR8opVl"},"source":["df_feature_imp = df_clean[['Glucose','BMI','Age','DiabetesPedigreeFunction','Outcome']]\n","df_feature_imp_name = df_feature_imp.columns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TCMyMGjZopVo"},"source":["X =  df_feature_imp[df_feature_imp_name[0 : df_feature_imp.shape[1] - 1]]\n","Y = df_feature_imp[df_feature_imp_name[df_feature_imp.shape[1] - 1]]\n","\n","X_train_im, X_test_im, y_train_im, y_test_im =train_test_split(X,Y,\n","                                                   test_size=0.1,\n","                                                   random_state=0,\n","                                                   stratify=df_feature_imp['Outcome'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uOla8b-qopVr"},"source":["models = GetScaledModel('minmax')\n","names,results = BasedLine2(X_train_im, y_train_im,models)\n","comparison = pd.DataFrame(np.array(results).T, columns=names)\n","print(comparison.head())\n","comparison.plot(kind='box')\n","\n","scaledScoreMinMax_im = ScoreDataFrame(names,results)\n","compareModels = pd.concat([basedLineScore,\n","                           scaledScoreStandard,\n","                          scaledScoreMinMax,\n","                          scaledScoreMinMax_c,\n","                          scaledScoreMinMax_im], axis=1)\n","compareModels"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sZZs7QzOopVu"},"source":["We still could improve the prediction"]},{"cell_type":"markdown","metadata":{"id":"WBVk_DxvopVv"},"source":["### Algortithm Tuning"]},{"cell_type":"code","metadata":{"id":"qGmiAgwxopVv"},"source":["df_unscaled = df_clean[['Glucose', 'BMI', 'Age', 'DiabetesPedigreeFunction', 'Outcome']]\n","df_imp_scaled_name = df_unscaled.columns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y5OtQae7opVz"},"source":["df_imp_scaled = MinMaxScaler().fit_transform(df_unscaled)\n","X =  df_imp_scaled[:, 0:4]\n","Y =  df_imp_scaled[:, 4]\n","X_train_sc, X_test_sc, y_train_sc, y_test_sc = train_test_split(X, Y,\n","                                                                test_size=0.1,\n","                                                                random_state=0,\n","                                                                stratify=df_imp_scaled[:, 4])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vyjye8F9opV2"},"source":["from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n","from scipy.stats import uniform"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m5juWBqGopV7"},"source":["class RandomSearch(object):\n","\n","    def __init__(self ,X_train, y_train, model, hyperparameters):\n","\n","        self.X_train = X_train\n","        self.y_train = y_train\n","        self.model = model\n","        self.hyperparameters = hyperparameters\n","\n","    def RandomSearch(self):\n","        # Create randomized search 10-fold cross validation and 100 iterations\n","        cv = 10\n","        clf = RandomizedSearchCV(self.model,\n","                                 self.hyperparameters,\n","                                 random_state=1,\n","                                 n_iter=100,\n","                                 cv=cv,\n","                                 verbose=0,\n","                                 n_jobs=-1,\n","                                 )\n","        # Fit randomized search\n","        best_model = clf.fit(self.X_train, self.y_train)\n","        message = (best_model.best_score_, best_model.best_params_)\n","        print(\"Best: %f using %s\" % (message))\n","\n","        return best_model, best_model.best_params_\n","\n","    def BestModelPridict(self, X_test):\n","\n","        best_model, _ = self.RandomSearch()\n","        pred = best_model.predict(X_test)\n","        return pred"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LaE0H9oSopV-"},"source":["class GridSearch(object):\n","\n","    def __init__(self, X_train, y_train, model, hyperparameters):\n","\n","        self.X_train = X_train\n","        self.y_train = y_train\n","        self.model = model\n","        self.hyperparameters = hyperparameters\n","\n","    def GridSearch(self):\n","        # Create randomized search 10-fold cross validation\n","        cv = 10\n","        clf = GridSearchCV(self.model,\n","                           self.hyperparameters,\n","                           cv=cv,\n","                           verbose=0,\n","                           n_jobs=-1,\n","                           )\n","\n","        # Fit grid search\n","        best_model = clf.fit(self.X_train, self.y_train)\n","        message = (best_model.best_score_, best_model.best_params_)\n","        print(\"Best: %f using %s\" % (message))\n","\n","        return best_model, best_model.best_params_\n","\n","    def BestModelPridict(self,X_test):\n","\n","        best_model, _ = self.GridSearch()\n","        pred = best_model.predict(X_test)\n","        return pred"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HNr3Gsd1opWC"},"source":["#### Logistic Regression\n","    - C : Regularization value, the more, the stronger the regularization(double).\n","    - RegularizationType: Can be either \"L2\" or “L1”. Default is “L2”."]},{"cell_type":"code","metadata":{"id":"rItrE7ykJcw1"},"source":["uniform(loc=0, scale=4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eWoXxpEtopWC"},"source":["# model\n","model = LogisticRegression()\n","# Create regularization penalty space\n","penalty = ['l1', 'l2']\n","\n","# Create regularization hyperparameter distribution using uniform distribution\n","C = uniform(loc=0, scale=4)\n","\n","# Create hyperparameter options\n","hyperparameters = dict(C=C, penalty=penalty)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T58zehNAJtNF"},"source":["hyperparameters"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8OyFtfwlopWG"},"source":["LR_RandSearch = RandomSearch(X_train_sc, y_train_sc, model, hyperparameters)\n","# LR_best_model, LR_best_params = LR_RandSearch.RandomSearch()\n","\n","Prediction_LR = LR_RandSearch.BestModelPridict(X_test_sc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AL3QebKHopWJ"},"source":["def floatingDecimals(f_val, dec=3):\n","  prc = \"{:.\"+str(dec)+\"f}\" #first cast decimal as str\n","  # print(prc) #str format output is {:.3f}\n","  return float(prc.format(f_val))\n","\n","print('prediction on test set is:', floatingDecimals((y_test_sc==Prediction_LR).mean(), 7))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4WVCzngEopWM"},"source":["### KNN\n","    - n_neighbors: Number of neighbors to use by default for k_neighbors queries"]},{"cell_type":"code","metadata":{"id":"YwxC6a5AopWM"},"source":["model_KNN = KNeighborsClassifier()\n","\n","neighbors = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n","param_grid = dict(n_neighbors=neighbors)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LTsBjPeYopWP"},"source":["KNN_GridSearch = GridSearch(X_train_sc, y_train_sc, model_KNN, param_grid)\n","Prediction_KNN = KNN_GridSearch.BestModelPridict(X_test_sc)\n","print('prediction on test set is:' ,floatingDecimals((y_test_sc == Prediction_KNN).mean(), 7))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vPhXjs7jopWR"},"source":["### SVC\n","    - C: The Penalty parameter C of the error term.\n","    - Kernel: Kernel type could be linear, poly, rbf or sigmoid."]},{"cell_type":"code","metadata":{"id":"Ra0j_rAgopWS"},"source":["c_values = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.3, 1.5, 1.7, 2.0]\n","kernel_values = [ 'linear' , 'poly' , 'rbf' , 'sigmoid' ]\n","param_grid = dict(C=c_values, kernel=kernel_values)\n","model_SVC = SVC()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K74Kb09-opWV"},"source":["SVC_GridSearch = GridSearch(X_train_sc,y_train_sc,model_SVC,param_grid)\n","Prediction_SVC = SVC_GridSearch.BestModelPridict(X_test_sc)\n","print('prediction on test set is:', floatingDecimals((y_test_sc == Prediction_SVC).mean(), 7))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PgC7br9TopWZ"},"source":["### Decision Tree\n","    - max_depth: Maximum depth of the tree (double).\n","    - row_subsample: Proportion of observations to consider (double).\n","    - max_features: Proportion of columns (features) to consider in each level (double)."]},{"cell_type":"code","metadata":{"id":"0L6Zxe4AopWZ"},"source":["from scipy.stats import randint\n","max_depth_value = [3, None]\n","max_features_value =  randint(1, 4)\n","min_samples_leaf_value = randint(1, 4)\n","criterion_value = [\"gini\", \"entropy\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0g2j_839opWd"},"source":["param_grid = dict(max_depth = max_depth_value,\n","                  max_features = max_features_value,\n","                  min_samples_leaf = min_samples_leaf_value,\n","                  criterion = criterion_value)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pJOT-SZtopWi"},"source":["model_CART = DecisionTreeClassifier()\n","CART_RandSearch = RandomSearch(X_train_sc,y_train_sc,model_CART,param_grid)\n","Prediction_CART = CART_RandSearch.BestModelPridict(X_test_sc)\n","print('prediction on test set is:' ,floatingDecimals((y_test_sc == Prediction_CART).mean(),7))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pehawWQ8opWl"},"source":["### AdaBoostClassifier\n","    - learning_rate: Learning rate shrinks the contribution of each classifier by learning_rate.\n","    - n_estimators: Number of trees to build."]},{"cell_type":"code","metadata":{"id":"b67vf_roopWl"},"source":["learning_rate_value = [.01,.05,.1,.5,1]\n","n_estimators_value = [50,100,150,200,250,300]\n","\n","param_grid = dict(learning_rate=learning_rate_value, n_estimators=n_estimators_value)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SrAeteGXopWn"},"source":["model_Ad = AdaBoostClassifier()\n","Ad_GridSearch = GridSearch(X_train_sc,y_train_sc,model_Ad,param_grid)\n","Prediction_Ad = Ad_GridSearch.BestModelPridict(X_test_sc)\n","print('prediction on test set is:' ,floatingDecimals((y_test_sc == Prediction_Ad).mean(),7))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C1V-R_z-opWp"},"source":["### GradientBoosting"]},{"cell_type":"code","metadata":{"id":"SsomgR1YopWp"},"source":["learning_rate_value = [.01,.05,.1,.5,1]\n","n_estimators_value = [50,100,150,200,250,300]\n","\n","param_grid = dict(learning_rate=learning_rate_value, n_estimators=n_estimators_value)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tmmxa92BopWr"},"source":["model_GB = GradientBoostingClassifier()\n","GB_GridSearch = GridSearch(X_train_sc,y_train_sc,model_GB,param_grid)\n","Prediction_GB = GB_GridSearch.BestModelPridict(X_test_sc)\n","print('prediction on test set is:' ,floatingDecimals((y_test_sc == Prediction_GB).mean(),7))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6nDMyu1_opWu"},"source":["### Ensemble Methods"]},{"cell_type":"markdown","metadata":{"id":"hJt2WqVHopWu"},"source":["### Voting Ensemble\n","Voting is one of the simplest ways of combining the predictions from multiple machine learning algorithms. It works by first creating two or more standalone models from your training dataset. A Voting Classifier can then be used to wrap your models and average the predictions of the sub-models when asked to make predictions for new data."]},{"cell_type":"code","metadata":{"id":"3zMi877popWu"},"source":["from sklearn.ensemble import VotingClassifier"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MC4rqihkopWx"},"source":["param = {'C': 0.7678243129497218, 'penalty': 'l2'}\n","model1 = LogisticRegression(**param)\n","\n","param = {'n_neighbors': 15}\n","model2 = KNeighborsClassifier(**param)\n","\n","param = {'C': 1.7, 'kernel': 'linear'}\n","model3 = SVC(**param)\n","\n","param = {'criterion': 'gini', 'max_depth': 3, 'max_features': 2, 'min_samples_leaf': 3}\n","model4 = DecisionTreeClassifier(**param)\n","\n","param = {'learning_rate': 0.05, 'n_estimators': 150}\n","model5 = AdaBoostClassifier(**param)\n","\n","param = {'learning_rate': 0.01, 'n_estimators': 100}\n","model6 = GradientBoostingClassifier(**param)\n","\n","model7 = GaussianNB()\n","\n","model8 = RandomForestClassifier()\n","\n","model9 = ExtraTreesClassifier()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"89RsWt8oopWz"},"source":["# create the sub models\n","estimators = [('LR',model1), ('KNN',model2), ('SVC',model3),\n","              ('DT',model4), ('ADa',model5), ('GB',model6),\n","              ('NB',model7), ('RF',model8),  ('ET',model9)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z-5N5cQsopW3"},"source":["# create the ensemble model\n","kfold = StratifiedKFold(n_splits=10, random_state=SEED, shuffle=True)\n","ensemble = VotingClassifier(estimators)\n","results = cross_val_score(ensemble, X_train_sc, y_train_sc, cv=kfold)\n","\n","print('Accuracy on train: ', results.mean())\n","\n","ensemble_model = ensemble.fit(X_train_sc, y_train_sc)\n","pred = ensemble.predict(X_test_sc)\n","\n","print('Accuracy on test:' , (y_test_sc == pred).mean())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4nKwZ5A4MbEd"},"source":[],"execution_count":null,"outputs":[]}]}