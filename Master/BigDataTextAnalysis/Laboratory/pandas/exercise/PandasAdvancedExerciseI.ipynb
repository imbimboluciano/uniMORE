{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"YJTcXfDtoXok"},"outputs":[],"source":["import pandas as pd\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"0NGe00gLoGGT"},"source":["# Load data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iT95Su8SfMjP"},"outputs":[],"source":["url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n","iris = pd.read_csv(url, header=None, names=['sepal_length','sepal_width', 'petal_length', 'petal_width', 'class'])\n","\n","iris.head()"]},{"cell_type":"markdown","metadata":{"id":"li7AGqDcFmvP"},"source":["# Data cleaning"]},{"cell_type":"markdown","metadata":{"id":"PKXon3a0otVB"},"source":["## Missing values"]},{"cell_type":"markdown","metadata":{"id":"Ei-GzjQKo1GG"},"source":["### Is there any missing value in the dataframe?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"49YVo0F4oI8B"},"outputs":[],"source":["iris.isnull().any().any()"]},{"cell_type":"markdown","metadata":{"id":"guRYVJd9pBtt"},"source":["### Lets set the values of the rows 10 to 29 of the column 'petal_length' to NaN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gKCk0LN1o7qG"},"outputs":[],"source":["iris.iloc[10:30,2] = np.nan"]},{"cell_type":"markdown","metadata":{"id":"EROqoxkZp7v5"},"source":["### Which column has the maximum number of missing values?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GlEv8ynap8NI"},"outputs":[],"source":["iris.columns[iris.isna().sum().argmax()]"]},{"cell_type":"markdown","metadata":{"id":"fF6PlMJDpK8t"},"source":["### Try to substitute the NaN values with two methods:\n","- replace null values with column mean (apply it to a copy of the dataframe)\n","- replace null values with 1.0\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_9boOzQUr0m3"},"outputs":[],"source":["iris_copy = iris.copy(deep=True)\n","iris_copy.fillna(iris_copy.mean(numeric_only=True))\n","\n","\n","iris.fillna(1.0, inplace=True)"]},{"cell_type":"markdown","metadata":{"id":"Df_h0mfTpaOZ"},"source":["### Set the first 3 rows as NaN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-T0eBFhWpXZG"},"outputs":[],"source":["iris.iloc[0:3,:] = np.nan\n","iris"]},{"cell_type":"markdown","metadata":{"id":"0tL8sTX4pf9y"},"source":["### Delete the rows that have all NaN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6fexAt6vpdC3"},"outputs":[],"source":["iris.dropna(how='all', inplace=True)"]},{"cell_type":"markdown","metadata":{"id":"CqU-2aKupl9N"},"source":["### Reset the index so it begins with 0 again"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zrq2CHACpjTc"},"outputs":[],"source":["iris.reindex([i for i in range(147)])"]},{"cell_type":"markdown","metadata":{"id":"B1dJ8XG-xLU8"},"source":["## Duplicates"]},{"cell_type":"markdown","metadata":{"id":"ahy9vSXPygTE"},"source":["### Does the dataframe contain duplicated rows? If any, visualize all duplicated rows (don't omit first or last occurrences)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Aq2iBuO3ylUY"},"outputs":[],"source":["iris_duplicate_rows = iris[iris.duplicated(keep=False)]\n","iris_duplicate_rows"]},{"cell_type":"markdown","metadata":{"id":"yJXRV5UG0CwG"},"source":["### Which row is the most repeated?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_U0w1KuIzqB9"},"outputs":[],"source":["iris_duplicate_rows.groupby(list(iris_duplicate_rows.columns.values)).size()"]},{"cell_type":"markdown","metadata":{"id":"6LbBZ_9NzbgZ"},"source":["### Drop duplicated rows"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rdXZ45HNxQW3"},"outputs":[],"source":["iris.drop_duplicates()"]},{"cell_type":"markdown","metadata":{"id":"O1ypIIt55RdA"},"source":["## Detect outliers, e.g., values that are higher than 85th percentile and lower than 25th percentile."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TvxMBue15WKN"},"outputs":[],"source":["iris_copy = iris.loc[:,'sepal_length':'petal_width'].copy()\n","iris_copy[(iris_copy > iris_copy.quantile(0.85)).all(axis=1)]\n","iris_copy[(iris_copy < iris_copy.quantile(0.25)).all(axis=1)]"]},{"cell_type":"markdown","metadata":{"id":"Vu2QLUx52HW2"},"source":["# Data transformation"]},{"cell_type":"markdown","metadata":{"id":"5CDeAKA92Mpf"},"source":["## Replace class values by removing \"Iris-\" prefix (use a dictionary)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Qw2H74gx5fy"},"outputs":[],"source":["map_classes = {\n","    'Iris-setosa': 'setosa',\n","    'Iris-versicolor': 'versicolor',\n","    'Iris-virginica': 'virginica'\n","}\n","\n","#class_map_values = {}\n","#for class_val in iris['class'].unique():\n","#  class_map_values[class_val] = class_val.replace(\"Iris-\", \"\")\n","\n","iris['class'] =  iris['class'].replace(map_classes)\n","iris"]},{"cell_type":"markdown","metadata":{"id":"PEaRJK9l4xQa"},"source":["## Delete columns\n","Delete for example class column"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5X0WD5Ia44DP"},"outputs":[],"source":["iris.drop('class',axis=1, inplace=True)"]},{"cell_type":"markdown","metadata":{"id":"MG4xuahg4fPo"},"source":["## How to normalize all columns in a dataframe?\n","- Normalize all columns of df by subtracting the column mean and divide by standard deviation.\n","- Range all columns of df such that the minimum value in each column is 0 and max is 1."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GaKy-1r74l6m"},"outputs":[],"source":["iris_mean = iris.mean(numeric_only=True)\n","iris_std = iris.std(numeric_only=True)\n","\n","out1 = ((iris - iris_mean) / iris_std).round(2)\n","out1\n","\n","# alternative\n","out1 = iris.apply(lambda x: ((x - x.mean())/x.std()).round(2))\n","out1\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["out2 = ((iris.max() - iris) / (iris.max() - iris.min())).round(2)\n","out2\n","\n","# alternative \n","out2 = iris.apply(lambda x: ((x.max() - x)/(x.max() - x.min())).round(2))\n","out2"]},{"cell_type":"markdown","metadata":{"id":"4fShsqiU38Aj"},"source":["## Binning and discretization\n","Discretize dataframe columns in 4 bins and get the new value frequency distribution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MSVo9tjS3QCV"},"outputs":[],"source":["discrete_iris = iris.apply(lambda x: pd.cut(x,4))\n","discrete_iris"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q0OPOWVwDbmn"},"outputs":[],"source":["\n","for i in range(4):\n","    print(discrete_iris.iloc[:,i].unique())\n","    print(discrete_iris.iloc[:,i].value_counts(normalize=True))"]},{"cell_type":"markdown","metadata":{"id":"Fh8HK9-BEXag"},"source":["## Binarize categorical data (dummy variables)\n","Based on the prevoius result, binarize all dataframe columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"duP9_h11Ds-X"},"outputs":[],"source":["pd.get_dummies(discrete_iris)"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":0}
